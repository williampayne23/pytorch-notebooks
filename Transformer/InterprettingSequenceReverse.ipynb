{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Decoder_Transformer import DecoderTransformerWrapper as dtw\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseDataset(Dataset):\n",
    "    def __init__(self, length):\n",
    "        self.length = length\n",
    "        self.size = 10**length # total number of possible combinations\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.randint(10, size=(self.length,), dtype=torch.long)\n",
    "        y = torch.flip(x,(-1,))\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "rd = ReverseDataset(6)\n",
    "\n",
    "batch_size = 2048\n",
    "train_loader = DataLoader(\n",
    "    rd, shuffle=True, pin_memory=True, batch_size=batch_size\n",
    ")\n",
    "device =torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformer_vocab():\n",
    "    def __init__(self, vocab):\n",
    "        \"\"\"Takes a list of unique words in vocab\"\"\"\n",
    "        self.vocab = vocab\n",
    "        self.indextoword = {i : word for i, word in enumerate(self.vocab)}\n",
    "        self.wordtoindex = {word: i for i, word in enumerate(self.vocab)} \n",
    "\n",
    "    def human_readable(self, l):\n",
    "        return [self.indextoword[i.item()] for i in l]\n",
    "\n",
    "    def machine_readable(self, l):\n",
    "        return [self.wordtoindex[i] for i in l]\n",
    "\n",
    "class transformer_trace():\n",
    "    def __init__(self, embeddings, attn, input, logits, vocab: transformer_vocab):\n",
    "        self.embeddings = embeddings\n",
    "        self.attn = attn\n",
    "        self.input = input.cpu()\n",
    "        self.logits = logits.cpu()\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def human_readable_input(self):\n",
    "        return [self.vocab.indextoword[i.item()] for i in self.input]\n",
    "\n",
    "    def get_attn(self, layer, head):\n",
    "        return self.attn[f'Layer {layer}'][0][head].cpu()\n",
    "\n",
    "    def plot_attn(self, layer, head):\n",
    "        plt.imshow(self.get_attn(layer=layer, head=head), interpolation='nearest')\n",
    "        plt.ylabel(\"Attention from\")\n",
    "        plt.xlabel(\"Attention to\")\n",
    "        plt.xticks(range(len(self.input)), self.vocab.human_readable(self.input))\n",
    "        plt.yticks(range(len(self.input)), self.vocab.human_readable(self.input))\n",
    "        plt.show()\n",
    "\n",
    "    def get_embeddings(self, layer):\n",
    "        return self.embeddings[f'Layer {layer}'][0].cpu()\n",
    "\n",
    "    def plot_embeddings(self, layer):\n",
    "        plt.imshow(self.get_embeddings(layer=layer), interpolation='nearest')\n",
    "        plt.ylabel(\"Sequence position\")\n",
    "        plt.xlabel(\"Token\")\n",
    "        plt.xticks(range(len(self.vocab.vocab)), self.vocab.vocab)\n",
    "        plt.yticks(range(len(self.input)))\n",
    "        plt.show()\n",
    "\n",
    "    def top_k_logits(self, k):\n",
    "        v, ix = torch.topk(self.logits, k, dim=-1)\n",
    "        out = self.logits.clone()\n",
    "        out[out < v] = -float('Inf')\n",
    "        self.logits = out\n",
    "        return self\n",
    "\n",
    "    def sample(self):\n",
    "        # apply softmax to convert to probabilities\n",
    "        probs = self.logits[0].softmax(dim=-1)\n",
    "        # sample from the distribution or take the most likely\n",
    "        ix = torch.multinomial(probs, num_samples=1)\n",
    "        return([self.vocab.indextoword[i.item()] for i in ix])\n",
    "\n",
    "    def __str__(self):\n",
    "        \" \".join(self.human_readable_input())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import math, copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        torch.zeros(d_model)\n",
    "        \n",
    "        encodings = torch.zeros(max_len, d_model)\n",
    "        dimensions = torch.arange(d_model)\n",
    "        denominator = div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        positions = torch.arange(max_len).unsqueeze(1)\n",
    "        encodings[:,0::2] = torch.sin(positions * denominator)\n",
    "        encodings[:,1::2] = torch.cos(positions * denominator)\n",
    "        self.register_buffer('encodings', encodings) #Registers a persistent buffer for this layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x; batch_size, seq_length, d_model\n",
    "        x = x + self.get_buffer('encodings')[:x.size(-2)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "def attention(Q, K, V, mask=None, dropout=None):\n",
    "    #Q, K, V; batch_size, h, seq_length, dk or dv\n",
    "    rt_d_k = math.sqrt(Q.size(-1))\n",
    "    scores = torch.matmul(Q, K.transpose(-1,-2)) / rt_d_k\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -math.inf)\n",
    "        \n",
    "    p_att = scores.softmax(dim=-1)\n",
    "    size = V.size()\n",
    "    if dropout is not None:\n",
    "        p_att = dropout(p_att)\n",
    "    \n",
    "    return torch.matmul(p_att, V), p_att\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, h = 8, mask=None):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.h = h\n",
    "        self.dk = d_model // h\n",
    "        self.dv = self.dk\n",
    "        self.attn = None\n",
    "        self.mask = mask\n",
    "        self.WQ = nn.Linear(d_model, d_model)\n",
    "        self.WK = nn.Linear(d_model, d_model)\n",
    "        self.WV = nn.Linear(d_model, d_model)\n",
    "        self.W0 = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # x; batch_size, seq_length, d_model\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        #Linearly project X into Q, K, and V\n",
    "        queries = self.WQ(x) #batch_size, seq_length, d_model\n",
    "        keys = self.WK(x)    #batch_size, seq_length, d_model\n",
    "        values = self.WV(x)  #batch_size, seq_length, d_model\n",
    "        \n",
    "        #Split Q, K, and V into multi headed\n",
    "        queries = queries.view(batch_size, -1, self.h, self.dk).transpose(1,2)\n",
    "        keys = keys.view(batch_size, -1, self.h, self.dk).transpose(1,2)\n",
    "        values = values.view(batch_size, -1, self.h, self.dv).transpose(1,2)\n",
    "        \n",
    "        #q,k,v ; batch_size, h, seq_length, (dk or dv)\n",
    "        \n",
    "        x, self.attn = attention(queries, keys, values, mask, self.dropout)\n",
    "        #x; batch_size, h, seq_length, dk\n",
    "        #self.attn; batch_size, \n",
    "        \n",
    "        x = x.transpose(1,2).contiguous().view(batch_size, -1, self.dk * self.h)\n",
    "        #x; batch_size, seq_length, model_size\n",
    "        \n",
    "        return self.W0(x)\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=2048, dropout=0.1):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.seq = nn.Sequential(nn.Linear(d_model, d_ff),\n",
    "                                 nn.ReLU(), \n",
    "                                 nn.Dropout(dropout),\n",
    "                                 nn.Linear(d_ff, d_model))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.seq(x)\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, hidden_layers=2048, dropout=0.1, h=8):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.mha = MultiHeadedAttention(d_model, dropout, h)\n",
    "        self.ffn = FeedForwardNetwork(d_model, hidden_layers, dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        base_mask = torch.tril(torch.ones(x.size(-2), x.size(-2))).to(x.device)\n",
    "        if mask is not None:\n",
    "            base_mask = mask*base_mask\n",
    "        x = self.ln1(x + self.mha(x, base_mask))\n",
    "        x = self.ln2(x + self.ffn(x))\n",
    "        return x\n",
    "\n",
    "class DecoderStack(nn.Module):\n",
    "    def __init__(self, d_model, N=8, hidden_layers=2048, dropout=0.1, h=8):\n",
    "        super(DecoderStack, self).__init__()\n",
    "        self.decoder_blocks = nn.ModuleList([copy.deepcopy(DecoderBlock(d_model, hidden_layers, dropout, h)) for x in range(N)])\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        for block in self.decoder_blocks:\n",
    "            x = block(x, mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderTransformer(nn.Module):\n",
    "    def __init__(self, d_model, vocab, N=8, d_ffl=2048, dropout=0.1, h=8, max_len=5000):\n",
    "        super(DecoderTransformer, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        self.learned_embeddings = nn.Embedding(vocab, d_model)\n",
    "        self.positional_embedding = PositionalEmbedding(d_model, dropout=dropout, max_len=max_len)\n",
    "        self.decoder_stack = DecoderStack(d_model, N=N, hidden_layers=d_ffl, dropout=dropout, h=h)\n",
    "        self.lin = nn.Linear(d_model, vocab)\n",
    "    \n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        #x; batch_size, seq_length, vocab_len\n",
    "        embedding = self.learned_embeddings(x) #batch_size, seq_length, d_model\n",
    "        pos_embedding = self.positional_embedding(embedding) #batch_size, seq_length, d_model\n",
    "        final_embedding = self.decoder_stack(pos_embedding) #batch_size, seq_length, d_model\n",
    "        logits = self.lin(final_embedding) #.softmax(dim=-1) #batch_size, seq_length, vocab_len\n",
    "        return logits\n",
    "        \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import math, copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "class DecoderTransformerHandler():\n",
    "    def __init__(self, model, vocab):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.decoder_transformer = model.to(self.device)\n",
    "        self.losses = []\n",
    "        self.vocab = vocab\n",
    "\n",
    "    @classmethod\n",
    "    def from_parameters(cls, d_model, vocab, d_ffl=None, height=4, heads=8, dropout=0.1, max_len=5000):\n",
    "        #vocab should be a list of allowed values\n",
    "        if d_ffl is None:\n",
    "            d_ffl = d_model * 4\n",
    "        model = DecoderTransformer(\n",
    "          d_model=d_model, \n",
    "          vocab=len(vocab.vocab), \n",
    "          N=height,\n",
    "          d_ffl=d_ffl, \n",
    "          dropout=dropout, \n",
    "          h=heads,\n",
    "          max_len=max_len\n",
    "        )\n",
    "        return cls(model, vocab)\n",
    "\n",
    "    def train(self, train_loader, lr=1e-4, max_epochs=1, loss_fn=nn.CrossEntropyLoss(), test_lambda=None):\n",
    "        self.decoder_transformer.train()\n",
    "        optimizer = optim.Adam(self.decoder_transformer.parameters(), lr=lr)\n",
    "        self.losses = []\n",
    "        for epoch in range(max_epochs):\n",
    "            pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "            for it, (x, y) in pbar:\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                logits = self.decoder_transformer(x)\n",
    "                loss = loss_fn(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "                loss.backward()\n",
    "                self.losses.append(loss.cpu().detach().numpy())\n",
    "\n",
    "                pred = logits.argmax(dim=-1)\n",
    "                if test_lambda is None:\n",
    "                    correct = (pred == y).type(torch.float).sum().item()\n",
    "                else:\n",
    "                    correct = test_lambda(pred, y).type(torch.float).sum().item()\n",
    "                accuracy = correct / x.size(0)\n",
    "                optimizer.step()\n",
    "                pbar.set_description(f\"epoch {epoch} iter {it}: train loss {loss.item():.5f}, accuracy {accuracy*100:0.2f}%\")\n",
    "  \n",
    "    def test(self, test_loader, test_lambda=None):\n",
    "        self.decoder_transformer.eval()\n",
    "        correct = 0\n",
    "        n = 0\n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(enumerate(test_loader), total=len(test_loader))\n",
    "            for it, (x, y) in pbar:\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "                n += x.size(0)\n",
    "                logits = self.decoder_transformer(x)\n",
    "                pred = logits.argmax(dim=-1)\n",
    "                if test_lambda is None:\n",
    "                    correct += (pred == y).type(torch.float).sum().item()\n",
    "                else:\n",
    "                    correct += test_lambda(pred, y).type(torch.float).sum().item()\n",
    "                accuracy = correct / n\n",
    "                pbar.set_description(f\"iter {it}: test accuracy {accuracy*100:.2f}%\")    \n",
    "    \n",
    "    def __call__(self, x):\n",
    "        self.decoder_transformer.eval()\n",
    "        x = x.to(self.device)\n",
    "        if len(x.size()) == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            logits = self.decoder_transformer(x)\n",
    "        return logits\n",
    "    \n",
    "    def interpreted_trace(self, tokens):\n",
    "        \"\"\"tokens is a sentence of tokens\"\"\"\n",
    "        x = torch.tensor([self.vocab.wordtoindex[token] for token in tokens])\n",
    "        embeddings = {}\n",
    "        attn = {}\n",
    "        def get_features(name):\n",
    "            def hook(model, input, output):\n",
    "                embeddings[name] = handler.decoder_transformer.lin(output.detach())\n",
    "                attn[name] = model.mha.attn.detach()\n",
    "            return hook\n",
    "        handles = [self.decoder_transformer.decoder_stack.decoder_blocks[i].register_forward_hook(get_features(f'Layer {i}')) for i in range(len(self.decoder_transformer.decoder_stack.decoder_blocks))]\n",
    "\n",
    "        logits = self(x)\n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "        \n",
    "        return transformer_trace(embeddings, attn, x, logits, self.vocab)\n",
    "\n",
    "    def to(self, device):\n",
    "        self.device = device\n",
    "        self.decoder_transformer.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 iter 488: train loss 1.51226, accuracy 40.80%: 100%|██████████| 489/489 [00:29<00:00, 16.57it/s]\n"
     ]
    }
   ],
   "source": [
    "handler = DecoderTransformerHandler.from_parameters(d_model=128,\n",
    "                                    vocab=transformer_vocab(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']),\n",
    "                                    height=1,\n",
    "                                    d_ffl=512, \n",
    "                                    dropout=0.1, \n",
    "                                    heads=2)\n",
    "handler.train(train_loader, test_lambda=lambda pred, y: (pred == y)[:, -3:].all(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = DecoderTransformerHandler(handler.decoder_transformer, transformer_vocab(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = handler.interpreted_trace(['a','b','c','d','e','f'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trace.plot_embeddings(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertviz import head_view, model_view\n",
    "from bertviz.neuron_view import show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\"></script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "      \n",
       "        <div id=\"bertviz-91e4a63ad31f42a08c374cfb9d29af62\" style=\"font-family:'Helvetica Neue', Helvetica, Arial, sans-serif;\">\n",
       "            <span style=\"user-select:none\">\n",
       "                Layer: <select id=\"layer\"></select>\n",
       "                \n",
       "            </span>\n",
       "            <div id='vis'></div>\n",
       "        </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "/**\n",
       " * @fileoverview Transformer Visualization D3 javascript code.\n",
       " *\n",
       " *\n",
       " *  Based on: https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/visualization/attention.js\n",
       " *\n",
       " * Change log:\n",
       " *\n",
       " * 12/19/18  Jesse Vig   Assorted cleanup. Changed orientation of attention matrices.\n",
       " * 12/29/20  Jesse Vig   Significant refactor.\n",
       " * 12/31/20  Jesse Vig   Support multiple visualizations in single notebook.\n",
       " * 02/06/21  Jesse Vig   Move require config from separate jupyter notebook step\n",
       " * 05/03/21  Jesse Vig   Adjust height of visualization dynamically\n",
       " * 07/25/21  Jesse Vig   Support layer filtering\n",
       " * 03/23/22  Daniel SC   Update requirement URLs for d3 and jQuery (source of bug not allowing end result to be displayed on browsers)\n",
       " **/\n",
       "\n",
       "require.config({\n",
       "  paths: {\n",
       "      d3: 'https://cdnjs.cloudflare.com/ajax/libs/d3/5.7.0/d3.min',\n",
       "    jquery: 'https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.0/jquery.min',\n",
       "  }\n",
       "});\n",
       "\n",
       "requirejs(['jquery', 'd3'], function ($, d3) {\n",
       "\n",
       "    const params = {\"attention\": [{\"name\": null, \"attn\": [[[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05118168145418167, 0.9488183259963989, 0.0, 0.0, 0.0, 0.0], [0.01079271174967289, 0.18000578880310059, 0.8092014789581299, 0.0, 0.0, 0.0], [0.0344904363155365, 0.15949785709381104, 0.45211759209632874, 0.35389411449432373, 0.0, 0.0], [0.29243409633636475, 0.277475506067276, 0.30028215050697327, 0.10574296861886978, 0.02406529150903225, 0.0], [0.7710908651351929, 0.1731548309326172, 0.04287806898355484, 0.007774324622005224, 0.0024878703989088535, 0.0026140103582292795]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.19669051468372345, 0.803309440612793, 0.0, 0.0, 0.0, 0.0], [0.022098936140537262, 0.18093889951705933, 0.7969622015953064, 0.0, 0.0, 0.0], [0.04574516415596008, 0.2456177920103073, 0.44699224829673767, 0.26164478063583374, 0.0, 0.0], [0.2234068214893341, 0.49708160758018494, 0.1835053563117981, 0.05637764558196068, 0.03962864354252815, 0.0], [0.5498625040054321, 0.3828129470348358, 0.04897324740886688, 0.009730055928230286, 0.004768582992255688, 0.0038526684511452913]]]], \"left_text\": [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"], \"right_text\": [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"]}], \"default_filter\": \"0\", \"root_div_id\": \"bertviz-91e4a63ad31f42a08c374cfb9d29af62\", \"layer\": null, \"heads\": null, \"include_layers\": [0]}; // HACK: {\"attention\": [{\"name\": null, \"attn\": [[[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05118168145418167, 0.9488183259963989, 0.0, 0.0, 0.0, 0.0], [0.01079271174967289, 0.18000578880310059, 0.8092014789581299, 0.0, 0.0, 0.0], [0.0344904363155365, 0.15949785709381104, 0.45211759209632874, 0.35389411449432373, 0.0, 0.0], [0.29243409633636475, 0.277475506067276, 0.30028215050697327, 0.10574296861886978, 0.02406529150903225, 0.0], [0.7710908651351929, 0.1731548309326172, 0.04287806898355484, 0.007774324622005224, 0.0024878703989088535, 0.0026140103582292795]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.19669051468372345, 0.803309440612793, 0.0, 0.0, 0.0, 0.0], [0.022098936140537262, 0.18093889951705933, 0.7969622015953064, 0.0, 0.0, 0.0], [0.04574516415596008, 0.2456177920103073, 0.44699224829673767, 0.26164478063583374, 0.0, 0.0], [0.2234068214893341, 0.49708160758018494, 0.1835053563117981, 0.05637764558196068, 0.03962864354252815, 0.0], [0.5498625040054321, 0.3828129470348358, 0.04897324740886688, 0.009730055928230286, 0.004768582992255688, 0.0038526684511452913]]]], \"left_text\": [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"], \"right_text\": [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"]}], \"default_filter\": \"0\", \"root_div_id\": \"bertviz-91e4a63ad31f42a08c374cfb9d29af62\", \"layer\": null, \"heads\": null, \"include_layers\": [0]} is a template marker that is replaced by actual params.\n",
       "    const TEXT_SIZE = 15;\n",
       "    const BOXWIDTH = 110;\n",
       "    const BOXHEIGHT = 22.5;\n",
       "    const MATRIX_WIDTH = 115;\n",
       "    const CHECKBOX_SIZE = 20;\n",
       "    const TEXT_TOP = 30;\n",
       "\n",
       "    console.log(\"d3 version\", d3.version)\n",
       "    let headColors;\n",
       "    try {\n",
       "        headColors = d3.scaleOrdinal(d3.schemeCategory10);\n",
       "    } catch (err) {\n",
       "        console.log('Older d3 version')\n",
       "        headColors = d3.scale.category10();\n",
       "    }\n",
       "    let config = {};\n",
       "    initialize();\n",
       "    renderVis();\n",
       "\n",
       "    function initialize() {\n",
       "        config.attention = params['attention'];\n",
       "        config.filter = params['default_filter'];\n",
       "        config.rootDivId = params['root_div_id'];\n",
       "        config.nLayers = config.attention[config.filter]['attn'].length;\n",
       "        config.nHeads = config.attention[config.filter]['attn'][0].length;\n",
       "        config.layers = params['include_layers']\n",
       "\n",
       "        if (params['heads']) {\n",
       "            config.headVis = new Array(config.nHeads).fill(false);\n",
       "            params['heads'].forEach(x => config.headVis[x] = true);\n",
       "        } else {\n",
       "            config.headVis = new Array(config.nHeads).fill(true);\n",
       "        }\n",
       "        config.initialTextLength = config.attention[config.filter].right_text.length;\n",
       "        config.layer_seq = (params['layer'] == null ? 0 : config.layers.findIndex(layer => params['layer'] === layer));\n",
       "        config.layer = config.layers[config.layer_seq]\n",
       "\n",
       "        let layerEl = $(`#${config.rootDivId} #layer`);\n",
       "        for (const layer of config.layers) {\n",
       "            layerEl.append($(\"<option />\").val(layer).text(layer));\n",
       "        }\n",
       "        layerEl.val(config.layer).change();\n",
       "        layerEl.on('change', function (e) {\n",
       "            config.layer = +e.currentTarget.value;\n",
       "            config.layer_seq = config.layers.findIndex(layer => config.layer === layer);\n",
       "            renderVis();\n",
       "        });\n",
       "\n",
       "        $(`#${config.rootDivId} #filter`).on('change', function (e) {\n",
       "            config.filter = e.currentTarget.value;\n",
       "            renderVis();\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function renderVis() {\n",
       "\n",
       "        // Load parameters\n",
       "        const attnData = config.attention[config.filter];\n",
       "        const leftText = attnData.left_text;\n",
       "        const rightText = attnData.right_text;\n",
       "\n",
       "        // Select attention for given layer\n",
       "        const layerAttention = attnData.attn[config.layer_seq];\n",
       "\n",
       "        // Clear vis\n",
       "        $(`#${config.rootDivId} #vis`).empty();\n",
       "\n",
       "        // Determine size of visualization\n",
       "        const height = Math.max(leftText.length, rightText.length) * BOXHEIGHT + TEXT_TOP;\n",
       "        const svg = d3.select(`#${config.rootDivId} #vis`)\n",
       "            .append('svg')\n",
       "            .attr(\"width\", \"100%\")\n",
       "            .attr(\"height\", height + \"px\");\n",
       "\n",
       "        // Display tokens on left and right side of visualization\n",
       "        renderText(svg, leftText, true, layerAttention, 0);\n",
       "        renderText(svg, rightText, false, layerAttention, MATRIX_WIDTH + BOXWIDTH);\n",
       "\n",
       "        // Render attention arcs\n",
       "        renderAttention(svg, layerAttention);\n",
       "\n",
       "        // Draw squares at top of visualization, one for each head\n",
       "        drawCheckboxes(0, svg, layerAttention);\n",
       "    }\n",
       "\n",
       "    function renderText(svg, text, isLeft, attention, leftPos) {\n",
       "\n",
       "        const textContainer = svg.append(\"svg:g\")\n",
       "            .attr(\"id\", isLeft ? \"left\" : \"right\");\n",
       "\n",
       "        // Add attention highlights superimposed over words\n",
       "        textContainer.append(\"g\")\n",
       "            .classed(\"attentionBoxes\", true)\n",
       "            .selectAll(\"g\")\n",
       "            .data(attention)\n",
       "            .enter()\n",
       "            .append(\"g\")\n",
       "            .attr(\"head-index\", (d, i) => i)\n",
       "            .selectAll(\"rect\")\n",
       "            .data(d => isLeft ? d : transpose(d)) // if right text, transpose attention to get right-to-left weights\n",
       "            .enter()\n",
       "            .append(\"rect\")\n",
       "            .attr(\"x\", function () {\n",
       "                var headIndex = +this.parentNode.getAttribute(\"head-index\");\n",
       "                return leftPos + boxOffsets(headIndex);\n",
       "            })\n",
       "            .attr(\"y\", (+1) * BOXHEIGHT)\n",
       "            .attr(\"width\", BOXWIDTH / activeHeads())\n",
       "            .attr(\"height\", BOXHEIGHT)\n",
       "            .attr(\"fill\", function () {\n",
       "                return headColors(+this.parentNode.getAttribute(\"head-index\"))\n",
       "            })\n",
       "            .style(\"opacity\", 0.0);\n",
       "\n",
       "        const tokenContainer = textContainer.append(\"g\").selectAll(\"g\")\n",
       "            .data(text)\n",
       "            .enter()\n",
       "            .append(\"g\");\n",
       "\n",
       "        // Add gray background that appears when hovering over text\n",
       "        tokenContainer.append(\"rect\")\n",
       "            .classed(\"background\", true)\n",
       "            .style(\"opacity\", 0.0)\n",
       "            .attr(\"fill\", \"lightgray\")\n",
       "            .attr(\"x\", leftPos)\n",
       "            .attr(\"y\", (d, i) => TEXT_TOP + i * BOXHEIGHT)\n",
       "            .attr(\"width\", BOXWIDTH)\n",
       "            .attr(\"height\", BOXHEIGHT);\n",
       "\n",
       "        // Add token text\n",
       "        const textEl = tokenContainer.append(\"text\")\n",
       "            .text(d => d)\n",
       "            .attr(\"font-size\", TEXT_SIZE + \"px\")\n",
       "            .style(\"cursor\", \"default\")\n",
       "            .style(\"-webkit-user-select\", \"none\")\n",
       "            .attr(\"x\", leftPos)\n",
       "            .attr(\"y\", (d, i) => TEXT_TOP + i * BOXHEIGHT);\n",
       "\n",
       "        if (isLeft) {\n",
       "            textEl.style(\"text-anchor\", \"end\")\n",
       "                .attr(\"dx\", BOXWIDTH - 0.5 * TEXT_SIZE)\n",
       "                .attr(\"dy\", TEXT_SIZE);\n",
       "        } else {\n",
       "            textEl.style(\"text-anchor\", \"start\")\n",
       "                .attr(\"dx\", +0.5 * TEXT_SIZE)\n",
       "                .attr(\"dy\", TEXT_SIZE);\n",
       "        }\n",
       "\n",
       "        tokenContainer.on(\"mouseover\", function (d, index) {\n",
       "\n",
       "            // Show gray background for moused-over token\n",
       "            textContainer.selectAll(\".background\")\n",
       "                .style(\"opacity\", (d, i) => i === index ? 1.0 : 0.0)\n",
       "\n",
       "            // Reset visibility attribute for any previously highlighted attention arcs\n",
       "            svg.select(\"#attention\")\n",
       "                .selectAll(\"line[visibility='visible']\")\n",
       "                .attr(\"visibility\", null)\n",
       "\n",
       "            // Hide group containing attention arcs\n",
       "            svg.select(\"#attention\").attr(\"visibility\", \"hidden\");\n",
       "\n",
       "            // Set to visible appropriate attention arcs to be highlighted\n",
       "            if (isLeft) {\n",
       "                svg.select(\"#attention\").selectAll(\"line[left-token-index='\" + index + \"']\").attr(\"visibility\", \"visible\");\n",
       "            } else {\n",
       "                svg.select(\"#attention\").selectAll(\"line[right-token-index='\" + index + \"']\").attr(\"visibility\", \"visible\");\n",
       "            }\n",
       "\n",
       "            // Update color boxes superimposed over tokens\n",
       "            const id = isLeft ? \"right\" : \"left\";\n",
       "            const leftPos = isLeft ? MATRIX_WIDTH + BOXWIDTH : 0;\n",
       "            svg.select(\"#\" + id)\n",
       "                .selectAll(\".attentionBoxes\")\n",
       "                .selectAll(\"g\")\n",
       "                .attr(\"head-index\", (d, i) => i)\n",
       "                .selectAll(\"rect\")\n",
       "                .attr(\"x\", function () {\n",
       "                    const headIndex = +this.parentNode.getAttribute(\"head-index\");\n",
       "                    return leftPos + boxOffsets(headIndex);\n",
       "                })\n",
       "                .attr(\"y\", (d, i) => TEXT_TOP + i * BOXHEIGHT)\n",
       "                .attr(\"width\", BOXWIDTH / activeHeads())\n",
       "                .attr(\"height\", BOXHEIGHT)\n",
       "                .style(\"opacity\", function (d) {\n",
       "                    const headIndex = +this.parentNode.getAttribute(\"head-index\");\n",
       "                    if (config.headVis[headIndex])\n",
       "                        if (d) {\n",
       "                            return d[index];\n",
       "                        } else {\n",
       "                            return 0.0;\n",
       "                        }\n",
       "                    else\n",
       "                        return 0.0;\n",
       "                });\n",
       "        });\n",
       "\n",
       "        textContainer.on(\"mouseleave\", function () {\n",
       "\n",
       "            // Unhighlight selected token\n",
       "            d3.select(this).selectAll(\".background\")\n",
       "                .style(\"opacity\", 0.0);\n",
       "\n",
       "            // Reset visibility attributes for previously selected lines\n",
       "            svg.select(\"#attention\")\n",
       "                .selectAll(\"line[visibility='visible']\")\n",
       "                .attr(\"visibility\", null) ;\n",
       "            svg.select(\"#attention\").attr(\"visibility\", \"visible\");\n",
       "\n",
       "            // Reset highlights superimposed over tokens\n",
       "            svg.selectAll(\".attentionBoxes\")\n",
       "                .selectAll(\"g\")\n",
       "                .selectAll(\"rect\")\n",
       "                .style(\"opacity\", 0.0);\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function renderAttention(svg, attention) {\n",
       "\n",
       "        // Remove previous dom elements\n",
       "        svg.select(\"#attention\").remove();\n",
       "\n",
       "        // Add new elements\n",
       "        svg.append(\"g\")\n",
       "            .attr(\"id\", \"attention\") // Container for all attention arcs\n",
       "            .selectAll(\".headAttention\")\n",
       "            .data(attention)\n",
       "            .enter()\n",
       "            .append(\"g\")\n",
       "            .classed(\"headAttention\", true) // Group attention arcs by head\n",
       "            .attr(\"head-index\", (d, i) => i)\n",
       "            .selectAll(\".tokenAttention\")\n",
       "            .data(d => d)\n",
       "            .enter()\n",
       "            .append(\"g\")\n",
       "            .classed(\"tokenAttention\", true) // Group attention arcs by left token\n",
       "            .attr(\"left-token-index\", (d, i) => i)\n",
       "            .selectAll(\"line\")\n",
       "            .data(d => d)\n",
       "            .enter()\n",
       "            .append(\"line\")\n",
       "            .attr(\"x1\", BOXWIDTH)\n",
       "            .attr(\"y1\", function () {\n",
       "                const leftTokenIndex = +this.parentNode.getAttribute(\"left-token-index\")\n",
       "                return TEXT_TOP + leftTokenIndex * BOXHEIGHT + (BOXHEIGHT / 2)\n",
       "            })\n",
       "            .attr(\"x2\", BOXWIDTH + MATRIX_WIDTH)\n",
       "            .attr(\"y2\", (d, rightTokenIndex) => TEXT_TOP + rightTokenIndex * BOXHEIGHT + (BOXHEIGHT / 2))\n",
       "            .attr(\"stroke-width\", 2)\n",
       "            .attr(\"stroke\", function () {\n",
       "                const headIndex = +this.parentNode.parentNode.getAttribute(\"head-index\");\n",
       "                return headColors(headIndex)\n",
       "            })\n",
       "            .attr(\"left-token-index\", function () {\n",
       "                return +this.parentNode.getAttribute(\"left-token-index\")\n",
       "            })\n",
       "            .attr(\"right-token-index\", (d, i) => i)\n",
       "        ;\n",
       "        updateAttention(svg)\n",
       "    }\n",
       "\n",
       "    function updateAttention(svg) {\n",
       "        svg.select(\"#attention\")\n",
       "            .selectAll(\"line\")\n",
       "            .attr(\"stroke-opacity\", function (d) {\n",
       "                const headIndex = +this.parentNode.parentNode.getAttribute(\"head-index\");\n",
       "                // If head is selected\n",
       "                if (config.headVis[headIndex]) {\n",
       "                    // Set opacity to attention weight divided by number of active heads\n",
       "                    return d / activeHeads()\n",
       "                } else {\n",
       "                    return 0.0;\n",
       "                }\n",
       "            })\n",
       "    }\n",
       "\n",
       "    function boxOffsets(i) {\n",
       "        const numHeadsAbove = config.headVis.reduce(\n",
       "            function (acc, val, cur) {\n",
       "                return val && cur < i ? acc + 1 : acc;\n",
       "            }, 0);\n",
       "        return numHeadsAbove * (BOXWIDTH / activeHeads());\n",
       "    }\n",
       "\n",
       "    function activeHeads() {\n",
       "        return config.headVis.reduce(function (acc, val) {\n",
       "            return val ? acc + 1 : acc;\n",
       "        }, 0);\n",
       "    }\n",
       "\n",
       "    function drawCheckboxes(top, svg) {\n",
       "        const checkboxContainer = svg.append(\"g\");\n",
       "        const checkbox = checkboxContainer.selectAll(\"rect\")\n",
       "            .data(config.headVis)\n",
       "            .enter()\n",
       "            .append(\"rect\")\n",
       "            .attr(\"fill\", (d, i) => headColors(i))\n",
       "            .attr(\"x\", (d, i) => i * CHECKBOX_SIZE)\n",
       "            .attr(\"y\", top)\n",
       "            .attr(\"width\", CHECKBOX_SIZE)\n",
       "            .attr(\"height\", CHECKBOX_SIZE);\n",
       "\n",
       "        function updateCheckboxes() {\n",
       "            checkboxContainer.selectAll(\"rect\")\n",
       "                .data(config.headVis)\n",
       "                .attr(\"fill\", (d, i) => d ? headColors(i): lighten(headColors(i)));\n",
       "        }\n",
       "\n",
       "        updateCheckboxes();\n",
       "\n",
       "        checkbox.on(\"click\", function (d, i) {\n",
       "            if (config.headVis[i] && activeHeads() === 1) return;\n",
       "            config.headVis[i] = !config.headVis[i];\n",
       "            updateCheckboxes();\n",
       "            updateAttention(svg);\n",
       "        });\n",
       "\n",
       "        checkbox.on(\"dblclick\", function (d, i) {\n",
       "            // If we double click on the only active head then reset\n",
       "            if (config.headVis[i] && activeHeads() === 1) {\n",
       "                config.headVis = new Array(config.nHeads).fill(true);\n",
       "            } else {\n",
       "                config.headVis = new Array(config.nHeads).fill(false);\n",
       "                config.headVis[i] = true;\n",
       "            }\n",
       "            updateCheckboxes();\n",
       "            updateAttention(svg);\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function lighten(color) {\n",
       "        const c = d3.hsl(color);\n",
       "        const increment = (1 - c.l) * 0.6;\n",
       "        c.l += increment;\n",
       "        c.s -= increment;\n",
       "        return c;\n",
       "    }\n",
       "\n",
       "    function transpose(mat) {\n",
       "        return mat[0].map(function (col, i) {\n",
       "            return mat.map(function (row) {\n",
       "                return row[i];\n",
       "            });\n",
       "        });\n",
       "    }\n",
       "\n",
       "});"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attention = trace.attn['Layer 0']\n",
    "tokens = trace.get_embeddings(0)\n",
    "input = trace.vocab.human_readable(trace.input)\n",
    "head_view((attention,), input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d', 'd', 'b', 'c', 'b', 'a']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46de579125ce40a59bc70aeeb47f0c79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "import torch\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2Model.from_pretrained(\"gpt2\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenizer\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a955a5d6d72dcc9422ec8add5a84d6ce2101e50b4aaf9a0a8386fcbc86cc6bec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
