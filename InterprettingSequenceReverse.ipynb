{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Decoder_Transformer import DecoderTransformerWrapper as dtw\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseDataset(Dataset):\n",
    "    def __init__(self, length):\n",
    "        self.length = length\n",
    "        self.size = 10**length # total number of possible combinations\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.randint(10, size=(self.length,), dtype=torch.long)\n",
    "        y = torch.flip(x,(-1,))\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "rd = ReverseDataset(6)\n",
    "\n",
    "batch_size = 2048\n",
    "train_loader = DataLoader(\n",
    "    rd, shuffle=True, pin_memory=True, batch_size=batch_size\n",
    ")\n",
    "device =torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformer_vocab():\n",
    "    def __init__(self, vocab):\n",
    "        \"\"\"Takes a list of unique words in vocab\"\"\"\n",
    "        self.vocab = vocab\n",
    "        self.indextoword = {i : word for i, word in enumerate(self.vocab)}\n",
    "        self.wordtoindex = {word: i for i, word in enumerate(self.vocab)} \n",
    "\n",
    "    def human_readable(self, l):\n",
    "        return [self.indextoword[i.item()] for i in l]\n",
    "\n",
    "    def machine_readable(self, l):\n",
    "        return [self.wordtoindex[i] for i in l]\n",
    "\n",
    "class transformer_trace():\n",
    "    def __init__(self, embeddings, attn, input, logits, vocab: transformer_vocab):\n",
    "        self.embeddings = embeddings\n",
    "        self.attn = attn\n",
    "        self.input = input.cpu()\n",
    "        self.logits = logits.cpu()\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def human_readable_input(self):\n",
    "        return [self.vocab.indextoword[i.item()] for i in self.input]\n",
    "\n",
    "    def get_attn(self, layer, head):\n",
    "        return self.attn[f'Layer {layer}'][0][head].cpu()\n",
    "\n",
    "    def plot_attn(self, layer, head):\n",
    "        plt.imshow(self.get_attn(layer=layer, head=head), interpolation='nearest')\n",
    "        plt.ylabel(\"Attention from\")\n",
    "        plt.xlabel(\"Attention to\")\n",
    "        plt.xticks(range(len(self.input)), self.vocab.human_readable(self.input))\n",
    "        plt.yticks(range(len(self.input)), self.vocab.human_readable(self.input))\n",
    "        plt.show()\n",
    "\n",
    "    def get_embeddings(self, layer):\n",
    "        return self.embeddings[f'Layer {layer}'][0].cpu()\n",
    "\n",
    "    def plot_embeddings(self, layer):\n",
    "        plt.imshow(self.get_embeddings(layer=layer), interpolation='nearest')\n",
    "        plt.ylabel(\"Sequence position\")\n",
    "        plt.xlabel(\"Token\")\n",
    "        plt.xticks(range(len(self.vocab.vocab)), self.vocab.vocab)\n",
    "        plt.yticks(range(len(self.input)))\n",
    "        plt.show()\n",
    "\n",
    "    def top_k_logits(self, k):\n",
    "        v, ix = torch.topk(self.logits, k, dim=-1)\n",
    "        out = self.logits.clone()\n",
    "        out[out < v] = -float('Inf')\n",
    "        self.logits = out\n",
    "        return self\n",
    "\n",
    "    def sample(self):\n",
    "        # apply softmax to convert to probabilities\n",
    "        probs = self.logits[0].softmax(dim=-1)\n",
    "        # sample from the distribution or take the most likely\n",
    "        ix = torch.multinomial(probs, num_samples=1)\n",
    "        return([self.vocab.indextoword[i.item()] for i in ix])\n",
    "\n",
    "    def __str__(self):\n",
    "        \" \".join(self.human_readable_input())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import math, copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        torch.zeros(d_model)\n",
    "        \n",
    "        encodings = torch.zeros(max_len, d_model)\n",
    "        dimensions = torch.arange(d_model)\n",
    "        denominator = div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        positions = torch.arange(max_len).unsqueeze(1)\n",
    "        encodings[:,0::2] = torch.sin(positions * denominator)\n",
    "        encodings[:,1::2] = torch.cos(positions * denominator)\n",
    "        self.register_buffer('encodings', encodings) #Registers a persistent buffer for this layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x; batch_size, seq_length, d_model\n",
    "        x = x + self.get_buffer('encodings')[:x.size(-2)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "def attention(Q, K, V, mask=None, dropout=None):\n",
    "    #Q, K, V; batch_size, h, seq_length, dk or dv\n",
    "    rt_d_k = math.sqrt(Q.size(-1))\n",
    "    scores = torch.matmul(Q, K.transpose(-1,-2)) / rt_d_k\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -math.inf)\n",
    "        \n",
    "    p_att = scores.softmax(dim=-1)\n",
    "    size = V.size()\n",
    "    if dropout is not None:\n",
    "        p_att = dropout(p_att)\n",
    "    \n",
    "    return torch.matmul(p_att, V), p_att\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, h = 8, mask=None):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.h = h\n",
    "        self.dk = d_model // h\n",
    "        self.dv = self.dk\n",
    "        self.attn = None\n",
    "        self.mask = mask\n",
    "        self.WQ = nn.Linear(d_model, d_model)\n",
    "        self.WK = nn.Linear(d_model, d_model)\n",
    "        self.WV = nn.Linear(d_model, d_model)\n",
    "        self.W0 = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # x; batch_size, seq_length, d_model\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        #Linearly project X into Q, K, and V\n",
    "        queries = self.WQ(x) #batch_size, seq_length, d_model\n",
    "        keys = self.WK(x)    #batch_size, seq_length, d_model\n",
    "        values = self.WV(x)  #batch_size, seq_length, d_model\n",
    "        \n",
    "        #Split Q, K, and V into multi headed\n",
    "        queries = queries.view(batch_size, -1, self.h, self.dk).transpose(1,2)\n",
    "        keys = keys.view(batch_size, -1, self.h, self.dk).transpose(1,2)\n",
    "        values = values.view(batch_size, -1, self.h, self.dv).transpose(1,2)\n",
    "        \n",
    "        #q,k,v ; batch_size, h, seq_length, (dk or dv)\n",
    "        \n",
    "        x, self.attn = attention(queries, keys, values, mask, self.dropout)\n",
    "        #x; batch_size, h, seq_length, dk\n",
    "        #self.attn; batch_size, \n",
    "        \n",
    "        x = x.transpose(1,2).contiguous().view(batch_size, -1, self.dk * self.h)\n",
    "        #x; batch_size, seq_length, model_size\n",
    "        \n",
    "        return self.W0(x)\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=2048, dropout=0.1):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.seq = nn.Sequential(nn.Linear(d_model, d_ff),\n",
    "                                 nn.ReLU(), \n",
    "                                 nn.Dropout(dropout),\n",
    "                                 nn.Linear(d_ff, d_model))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.seq(x)\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, hidden_layers=2048, dropout=0.1, h=8):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.mha = MultiHeadedAttention(d_model, dropout, h)\n",
    "        self.ffn = FeedForwardNetwork(d_model, hidden_layers, dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        base_mask = torch.tril(torch.ones(x.size(-2), x.size(-2))).to(x.device)\n",
    "        if mask is not None:\n",
    "            base_mask = mask*base_mask\n",
    "        x = self.ln1(x + self.mha(x, base_mask))\n",
    "        x = self.ln2(x + self.ffn(x))\n",
    "        return x\n",
    "\n",
    "class DecoderStack(nn.Module):\n",
    "    def __init__(self, d_model, N=8, hidden_layers=2048, dropout=0.1, h=8):\n",
    "        super(DecoderStack, self).__init__()\n",
    "        self.decoder_blocks = nn.ModuleList([copy.deepcopy(DecoderBlock(d_model, hidden_layers, dropout, h)) for x in range(N)])\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        for block in self.decoder_blocks:\n",
    "            x = block(x, mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderTransformer(nn.Module):\n",
    "    def __init__(self, d_model, vocab, N=8, d_ffl=2048, dropout=0.1, h=8, max_len=5000):\n",
    "        super(DecoderTransformer, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        self.learned_embeddings = nn.Embedding(vocab, d_model)\n",
    "        self.positional_embedding = PositionalEmbedding(d_model, dropout=dropout, max_len=max_len)\n",
    "        self.decoder_stack = DecoderStack(d_model, N=N, hidden_layers=d_ffl, dropout=dropout, h=h)\n",
    "        self.lin = nn.Linear(d_model, vocab)\n",
    "    \n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        #x; batch_size, seq_length, vocab_len\n",
    "        embedding = self.learned_embeddings(x) #batch_size, seq_length, d_model\n",
    "        pos_embedding = self.positional_embedding(embedding) #batch_size, seq_length, d_model\n",
    "        final_embedding = self.decoder_stack(pos_embedding) #batch_size, seq_length, d_model\n",
    "        logits = self.lin(final_embedding) #.softmax(dim=-1) #batch_size, seq_length, vocab_len\n",
    "        return logits\n",
    "        \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import math, copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "class DecoderTransformerHandler():\n",
    "    def __init__(self, model, vocab):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.decoder_transformer = model.to(self.device)\n",
    "        self.losses = []\n",
    "        self.vocab = vocab\n",
    "\n",
    "    @classmethod\n",
    "    def from_parameters(cls, d_model, vocab, d_ffl=None, height=4, heads=8, dropout=0.1, max_len=5000):\n",
    "        #vocab should be a list of allowed values\n",
    "        if d_ffl is None:\n",
    "            d_ffl = d_model * 4\n",
    "        model = DecoderTransformer(\n",
    "          d_model=d_model, \n",
    "          vocab=len(vocab.vocab), \n",
    "          N=height,\n",
    "          d_ffl=d_ffl, \n",
    "          dropout=dropout, \n",
    "          h=heads,\n",
    "          max_len=max_len\n",
    "        )\n",
    "        return cls(model, vocab)\n",
    "\n",
    "    def train(self, train_loader, lr=1e-4, max_epochs=1, loss_fn=nn.CrossEntropyLoss(), test_lambda=None):\n",
    "        self.decoder_transformer.train()\n",
    "        optimizer = optim.Adam(self.decoder_transformer.parameters(), lr=lr)\n",
    "        self.losses = []\n",
    "        for epoch in range(max_epochs):\n",
    "            pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "            for it, (x, y) in pbar:\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                logits = self.decoder_transformer(x)\n",
    "                loss = loss_fn(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "                loss.backward()\n",
    "                self.losses.append(loss.cpu().detach().numpy())\n",
    "\n",
    "                pred = logits.argmax(dim=-1)\n",
    "                if test_lambda is None:\n",
    "                    correct = (pred == y).type(torch.float).sum().item()\n",
    "                else:\n",
    "                    correct = test_lambda(pred, y).type(torch.float).sum().item()\n",
    "                accuracy = correct / x.size(0)\n",
    "                optimizer.step()\n",
    "                pbar.set_description(f\"epoch {epoch} iter {it}: train loss {loss.item():.5f}, accuracy {accuracy*100:0.2f}%\")\n",
    "  \n",
    "    def test(self, test_loader, test_lambda=None):\n",
    "        self.decoder_transformer.eval()\n",
    "        correct = 0\n",
    "        n = 0\n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(enumerate(test_loader), total=len(test_loader))\n",
    "            for it, (x, y) in pbar:\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "                n += x.size(0)\n",
    "                logits = self.decoder_transformer(x)\n",
    "                pred = logits.argmax(dim=-1)\n",
    "                if test_lambda is None:\n",
    "                    correct += (pred == y).type(torch.float).sum().item()\n",
    "                else:\n",
    "                    correct += test_lambda(pred, y).type(torch.float).sum().item()\n",
    "                accuracy = correct / n\n",
    "                pbar.set_description(f\"iter {it}: test accuracy {accuracy*100:.2f}%\")    \n",
    "    \n",
    "    def __call__(self, x):\n",
    "        self.decoder_transformer.eval()\n",
    "        x = x.to(self.device)\n",
    "        if len(x.size()) == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            logits = self.decoder_transformer(x)\n",
    "        return logits\n",
    "    \n",
    "    def interpreted_trace(self, tokens):\n",
    "        \"\"\"tokens is a sentence of tokens\"\"\"\n",
    "        x = torch.tensor([self.vocab.wordtoindex[token] for token in tokens])\n",
    "        embeddings = {}\n",
    "        attn = {}\n",
    "        def get_features(name):\n",
    "            def hook(model, input, output):\n",
    "                tokens[name] = handler.decoder_transformer.lin(output.detach())\n",
    "                attn[name] = model.mha.attn.detach()\n",
    "            return hook\n",
    "        handles = [self.decoder_transformer.decoder_stack.decoder_blocks[i].register_forward_hook(get_features(f'Layer {i}')) for i in range(len(self.decoder_transformer.decoder_stack.decoder_blocks))]\n",
    "\n",
    "        logits = self(x)\n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "        \n",
    "        return transformer_trace(embeddings, attn, x, logits, self.vocab)\n",
    "\n",
    "    def to(self, device):\n",
    "        self.device = device\n",
    "        self.decoder_transformer.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 iter 488: train loss 1.46381, accuracy 46.01%: 100%|██████████| 489/489 [00:22<00:00, 22.06it/s]\n"
     ]
    }
   ],
   "source": [
    "handler = DecoderTransformerHandler.from_parameters(d_model=128,\n",
    "                                    vocab=transformer_vocab(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']),\n",
    "                                    height=1,\n",
    "                                    d_ffl=512, \n",
    "                                    dropout=0.1, \n",
    "                                    heads=2)\n",
    "handler.train(train_loader, test_lambda=lambda pred, y: (pred == y)[:, -3:].all(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = DecoderTransformerHandler(handler.decoder_transformer, transformer_vocab(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = handler.interpreted_trace(['a','b','c','d','e','f'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trace.plot_embeddings(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertviz import head_view, model_view\n",
    "from bertviz.neuron_view import show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\"></script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "      \n",
       "        <div id=\"bertviz-91e4a63ad31f42a08c374cfb9d29af62\" style=\"font-family:'Helvetica Neue', Helvetica, Arial, sans-serif;\">\n",
       "            <span style=\"user-select:none\">\n",
       "                Layer: <select id=\"layer\"></select>\n",
       "                \n",
       "            </span>\n",
       "            <div id='vis'></div>\n",
       "        </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "/**\n",
       " * @fileoverview Transformer Visualization D3 javascript code.\n",
       " *\n",
       " *\n",
       " *  Based on: https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/visualization/attention.js\n",
       " *\n",
       " * Change log:\n",
       " *\n",
       " * 12/19/18  Jesse Vig   Assorted cleanup. Changed orientation of attention matrices.\n",
       " * 12/29/20  Jesse Vig   Significant refactor.\n",
       " * 12/31/20  Jesse Vig   Support multiple visualizations in single notebook.\n",
       " * 02/06/21  Jesse Vig   Move require config from separate jupyter notebook step\n",
       " * 05/03/21  Jesse Vig   Adjust height of visualization dynamically\n",
       " * 07/25/21  Jesse Vig   Support layer filtering\n",
       " * 03/23/22  Daniel SC   Update requirement URLs for d3 and jQuery (source of bug not allowing end result to be displayed on browsers)\n",
       " **/\n",
       "\n",
       "require.config({\n",
       "  paths: {\n",
       "      d3: 'https://cdnjs.cloudflare.com/ajax/libs/d3/5.7.0/d3.min',\n",
       "    jquery: 'https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.0/jquery.min',\n",
       "  }\n",
       "});\n",
       "\n",
       "requirejs(['jquery', 'd3'], function ($, d3) {\n",
       "\n",
       "    const params = {\"attention\": [{\"name\": null, \"attn\": [[[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05118168145418167, 0.9488183259963989, 0.0, 0.0, 0.0, 0.0], [0.01079271174967289, 0.18000578880310059, 0.8092014789581299, 0.0, 0.0, 0.0], [0.0344904363155365, 0.15949785709381104, 0.45211759209632874, 0.35389411449432373, 0.0, 0.0], [0.29243409633636475, 0.277475506067276, 0.30028215050697327, 0.10574296861886978, 0.02406529150903225, 0.0], [0.7710908651351929, 0.1731548309326172, 0.04287806898355484, 0.007774324622005224, 0.0024878703989088535, 0.0026140103582292795]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.19669051468372345, 0.803309440612793, 0.0, 0.0, 0.0, 0.0], [0.022098936140537262, 0.18093889951705933, 0.7969622015953064, 0.0, 0.0, 0.0], [0.04574516415596008, 0.2456177920103073, 0.44699224829673767, 0.26164478063583374, 0.0, 0.0], [0.2234068214893341, 0.49708160758018494, 0.1835053563117981, 0.05637764558196068, 0.03962864354252815, 0.0], [0.5498625040054321, 0.3828129470348358, 0.04897324740886688, 0.009730055928230286, 0.004768582992255688, 0.0038526684511452913]]]], \"left_text\": [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"], \"right_text\": [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"]}], \"default_filter\": \"0\", \"root_div_id\": \"bertviz-91e4a63ad31f42a08c374cfb9d29af62\", \"layer\": null, \"heads\": null, \"include_layers\": [0]}; // HACK: {\"attention\": [{\"name\": null, \"attn\": [[[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05118168145418167, 0.9488183259963989, 0.0, 0.0, 0.0, 0.0], [0.01079271174967289, 0.18000578880310059, 0.8092014789581299, 0.0, 0.0, 0.0], [0.0344904363155365, 0.15949785709381104, 0.45211759209632874, 0.35389411449432373, 0.0, 0.0], [0.29243409633636475, 0.277475506067276, 0.30028215050697327, 0.10574296861886978, 0.02406529150903225, 0.0], [0.7710908651351929, 0.1731548309326172, 0.04287806898355484, 0.007774324622005224, 0.0024878703989088535, 0.0026140103582292795]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.19669051468372345, 0.803309440612793, 0.0, 0.0, 0.0, 0.0], [0.022098936140537262, 0.18093889951705933, 0.7969622015953064, 0.0, 0.0, 0.0], [0.04574516415596008, 0.2456177920103073, 0.44699224829673767, 0.26164478063583374, 0.0, 0.0], [0.2234068214893341, 0.49708160758018494, 0.1835053563117981, 0.05637764558196068, 0.03962864354252815, 0.0], [0.5498625040054321, 0.3828129470348358, 0.04897324740886688, 0.009730055928230286, 0.004768582992255688, 0.0038526684511452913]]]], \"left_text\": [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"], \"right_text\": [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"]}], \"default_filter\": \"0\", \"root_div_id\": \"bertviz-91e4a63ad31f42a08c374cfb9d29af62\", \"layer\": null, \"heads\": null, \"include_layers\": [0]} is a template marker that is replaced by actual params.\n",
       "    const TEXT_SIZE = 15;\n",
       "    const BOXWIDTH = 110;\n",
       "    const BOXHEIGHT = 22.5;\n",
       "    const MATRIX_WIDTH = 115;\n",
       "    const CHECKBOX_SIZE = 20;\n",
       "    const TEXT_TOP = 30;\n",
       "\n",
       "    console.log(\"d3 version\", d3.version)\n",
       "    let headColors;\n",
       "    try {\n",
       "        headColors = d3.scaleOrdinal(d3.schemeCategory10);\n",
       "    } catch (err) {\n",
       "        console.log('Older d3 version')\n",
       "        headColors = d3.scale.category10();\n",
       "    }\n",
       "    let config = {};\n",
       "    initialize();\n",
       "    renderVis();\n",
       "\n",
       "    function initialize() {\n",
       "        config.attention = params['attention'];\n",
       "        config.filter = params['default_filter'];\n",
       "        config.rootDivId = params['root_div_id'];\n",
       "        config.nLayers = config.attention[config.filter]['attn'].length;\n",
       "        config.nHeads = config.attention[config.filter]['attn'][0].length;\n",
       "        config.layers = params['include_layers']\n",
       "\n",
       "        if (params['heads']) {\n",
       "            config.headVis = new Array(config.nHeads).fill(false);\n",
       "            params['heads'].forEach(x => config.headVis[x] = true);\n",
       "        } else {\n",
       "            config.headVis = new Array(config.nHeads).fill(true);\n",
       "        }\n",
       "        config.initialTextLength = config.attention[config.filter].right_text.length;\n",
       "        config.layer_seq = (params['layer'] == null ? 0 : config.layers.findIndex(layer => params['layer'] === layer));\n",
       "        config.layer = config.layers[config.layer_seq]\n",
       "\n",
       "        let layerEl = $(`#${config.rootDivId} #layer`);\n",
       "        for (const layer of config.layers) {\n",
       "            layerEl.append($(\"<option />\").val(layer).text(layer));\n",
       "        }\n",
       "        layerEl.val(config.layer).change();\n",
       "        layerEl.on('change', function (e) {\n",
       "            config.layer = +e.currentTarget.value;\n",
       "            config.layer_seq = config.layers.findIndex(layer => config.layer === layer);\n",
       "            renderVis();\n",
       "        });\n",
       "\n",
       "        $(`#${config.rootDivId} #filter`).on('change', function (e) {\n",
       "            config.filter = e.currentTarget.value;\n",
       "            renderVis();\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function renderVis() {\n",
       "\n",
       "        // Load parameters\n",
       "        const attnData = config.attention[config.filter];\n",
       "        const leftText = attnData.left_text;\n",
       "        const rightText = attnData.right_text;\n",
       "\n",
       "        // Select attention for given layer\n",
       "        const layerAttention = attnData.attn[config.layer_seq];\n",
       "\n",
       "        // Clear vis\n",
       "        $(`#${config.rootDivId} #vis`).empty();\n",
       "\n",
       "        // Determine size of visualization\n",
       "        const height = Math.max(leftText.length, rightText.length) * BOXHEIGHT + TEXT_TOP;\n",
       "        const svg = d3.select(`#${config.rootDivId} #vis`)\n",
       "            .append('svg')\n",
       "            .attr(\"width\", \"100%\")\n",
       "            .attr(\"height\", height + \"px\");\n",
       "\n",
       "        // Display tokens on left and right side of visualization\n",
       "        renderText(svg, leftText, true, layerAttention, 0);\n",
       "        renderText(svg, rightText, false, layerAttention, MATRIX_WIDTH + BOXWIDTH);\n",
       "\n",
       "        // Render attention arcs\n",
       "        renderAttention(svg, layerAttention);\n",
       "\n",
       "        // Draw squares at top of visualization, one for each head\n",
       "        drawCheckboxes(0, svg, layerAttention);\n",
       "    }\n",
       "\n",
       "    function renderText(svg, text, isLeft, attention, leftPos) {\n",
       "\n",
       "        const textContainer = svg.append(\"svg:g\")\n",
       "            .attr(\"id\", isLeft ? \"left\" : \"right\");\n",
       "\n",
       "        // Add attention highlights superimposed over words\n",
       "        textContainer.append(\"g\")\n",
       "            .classed(\"attentionBoxes\", true)\n",
       "            .selectAll(\"g\")\n",
       "            .data(attention)\n",
       "            .enter()\n",
       "            .append(\"g\")\n",
       "            .attr(\"head-index\", (d, i) => i)\n",
       "            .selectAll(\"rect\")\n",
       "            .data(d => isLeft ? d : transpose(d)) // if right text, transpose attention to get right-to-left weights\n",
       "            .enter()\n",
       "            .append(\"rect\")\n",
       "            .attr(\"x\", function () {\n",
       "                var headIndex = +this.parentNode.getAttribute(\"head-index\");\n",
       "                return leftPos + boxOffsets(headIndex);\n",
       "            })\n",
       "            .attr(\"y\", (+1) * BOXHEIGHT)\n",
       "            .attr(\"width\", BOXWIDTH / activeHeads())\n",
       "            .attr(\"height\", BOXHEIGHT)\n",
       "            .attr(\"fill\", function () {\n",
       "                return headColors(+this.parentNode.getAttribute(\"head-index\"))\n",
       "            })\n",
       "            .style(\"opacity\", 0.0);\n",
       "\n",
       "        const tokenContainer = textContainer.append(\"g\").selectAll(\"g\")\n",
       "            .data(text)\n",
       "            .enter()\n",
       "            .append(\"g\");\n",
       "\n",
       "        // Add gray background that appears when hovering over text\n",
       "        tokenContainer.append(\"rect\")\n",
       "            .classed(\"background\", true)\n",
       "            .style(\"opacity\", 0.0)\n",
       "            .attr(\"fill\", \"lightgray\")\n",
       "            .attr(\"x\", leftPos)\n",
       "            .attr(\"y\", (d, i) => TEXT_TOP + i * BOXHEIGHT)\n",
       "            .attr(\"width\", BOXWIDTH)\n",
       "            .attr(\"height\", BOXHEIGHT);\n",
       "\n",
       "        // Add token text\n",
       "        const textEl = tokenContainer.append(\"text\")\n",
       "            .text(d => d)\n",
       "            .attr(\"font-size\", TEXT_SIZE + \"px\")\n",
       "            .style(\"cursor\", \"default\")\n",
       "            .style(\"-webkit-user-select\", \"none\")\n",
       "            .attr(\"x\", leftPos)\n",
       "            .attr(\"y\", (d, i) => TEXT_TOP + i * BOXHEIGHT);\n",
       "\n",
       "        if (isLeft) {\n",
       "            textEl.style(\"text-anchor\", \"end\")\n",
       "                .attr(\"dx\", BOXWIDTH - 0.5 * TEXT_SIZE)\n",
       "                .attr(\"dy\", TEXT_SIZE);\n",
       "        } else {\n",
       "            textEl.style(\"text-anchor\", \"start\")\n",
       "                .attr(\"dx\", +0.5 * TEXT_SIZE)\n",
       "                .attr(\"dy\", TEXT_SIZE);\n",
       "        }\n",
       "\n",
       "        tokenContainer.on(\"mouseover\", function (d, index) {\n",
       "\n",
       "            // Show gray background for moused-over token\n",
       "            textContainer.selectAll(\".background\")\n",
       "                .style(\"opacity\", (d, i) => i === index ? 1.0 : 0.0)\n",
       "\n",
       "            // Reset visibility attribute for any previously highlighted attention arcs\n",
       "            svg.select(\"#attention\")\n",
       "                .selectAll(\"line[visibility='visible']\")\n",
       "                .attr(\"visibility\", null)\n",
       "\n",
       "            // Hide group containing attention arcs\n",
       "            svg.select(\"#attention\").attr(\"visibility\", \"hidden\");\n",
       "\n",
       "            // Set to visible appropriate attention arcs to be highlighted\n",
       "            if (isLeft) {\n",
       "                svg.select(\"#attention\").selectAll(\"line[left-token-index='\" + index + \"']\").attr(\"visibility\", \"visible\");\n",
       "            } else {\n",
       "                svg.select(\"#attention\").selectAll(\"line[right-token-index='\" + index + \"']\").attr(\"visibility\", \"visible\");\n",
       "            }\n",
       "\n",
       "            // Update color boxes superimposed over tokens\n",
       "            const id = isLeft ? \"right\" : \"left\";\n",
       "            const leftPos = isLeft ? MATRIX_WIDTH + BOXWIDTH : 0;\n",
       "            svg.select(\"#\" + id)\n",
       "                .selectAll(\".attentionBoxes\")\n",
       "                .selectAll(\"g\")\n",
       "                .attr(\"head-index\", (d, i) => i)\n",
       "                .selectAll(\"rect\")\n",
       "                .attr(\"x\", function () {\n",
       "                    const headIndex = +this.parentNode.getAttribute(\"head-index\");\n",
       "                    return leftPos + boxOffsets(headIndex);\n",
       "                })\n",
       "                .attr(\"y\", (d, i) => TEXT_TOP + i * BOXHEIGHT)\n",
       "                .attr(\"width\", BOXWIDTH / activeHeads())\n",
       "                .attr(\"height\", BOXHEIGHT)\n",
       "                .style(\"opacity\", function (d) {\n",
       "                    const headIndex = +this.parentNode.getAttribute(\"head-index\");\n",
       "                    if (config.headVis[headIndex])\n",
       "                        if (d) {\n",
       "                            return d[index];\n",
       "                        } else {\n",
       "                            return 0.0;\n",
       "                        }\n",
       "                    else\n",
       "                        return 0.0;\n",
       "                });\n",
       "        });\n",
       "\n",
       "        textContainer.on(\"mouseleave\", function () {\n",
       "\n",
       "            // Unhighlight selected token\n",
       "            d3.select(this).selectAll(\".background\")\n",
       "                .style(\"opacity\", 0.0);\n",
       "\n",
       "            // Reset visibility attributes for previously selected lines\n",
       "            svg.select(\"#attention\")\n",
       "                .selectAll(\"line[visibility='visible']\")\n",
       "                .attr(\"visibility\", null) ;\n",
       "            svg.select(\"#attention\").attr(\"visibility\", \"visible\");\n",
       "\n",
       "            // Reset highlights superimposed over tokens\n",
       "            svg.selectAll(\".attentionBoxes\")\n",
       "                .selectAll(\"g\")\n",
       "                .selectAll(\"rect\")\n",
       "                .style(\"opacity\", 0.0);\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function renderAttention(svg, attention) {\n",
       "\n",
       "        // Remove previous dom elements\n",
       "        svg.select(\"#attention\").remove();\n",
       "\n",
       "        // Add new elements\n",
       "        svg.append(\"g\")\n",
       "            .attr(\"id\", \"attention\") // Container for all attention arcs\n",
       "            .selectAll(\".headAttention\")\n",
       "            .data(attention)\n",
       "            .enter()\n",
       "            .append(\"g\")\n",
       "            .classed(\"headAttention\", true) // Group attention arcs by head\n",
       "            .attr(\"head-index\", (d, i) => i)\n",
       "            .selectAll(\".tokenAttention\")\n",
       "            .data(d => d)\n",
       "            .enter()\n",
       "            .append(\"g\")\n",
       "            .classed(\"tokenAttention\", true) // Group attention arcs by left token\n",
       "            .attr(\"left-token-index\", (d, i) => i)\n",
       "            .selectAll(\"line\")\n",
       "            .data(d => d)\n",
       "            .enter()\n",
       "            .append(\"line\")\n",
       "            .attr(\"x1\", BOXWIDTH)\n",
       "            .attr(\"y1\", function () {\n",
       "                const leftTokenIndex = +this.parentNode.getAttribute(\"left-token-index\")\n",
       "                return TEXT_TOP + leftTokenIndex * BOXHEIGHT + (BOXHEIGHT / 2)\n",
       "            })\n",
       "            .attr(\"x2\", BOXWIDTH + MATRIX_WIDTH)\n",
       "            .attr(\"y2\", (d, rightTokenIndex) => TEXT_TOP + rightTokenIndex * BOXHEIGHT + (BOXHEIGHT / 2))\n",
       "            .attr(\"stroke-width\", 2)\n",
       "            .attr(\"stroke\", function () {\n",
       "                const headIndex = +this.parentNode.parentNode.getAttribute(\"head-index\");\n",
       "                return headColors(headIndex)\n",
       "            })\n",
       "            .attr(\"left-token-index\", function () {\n",
       "                return +this.parentNode.getAttribute(\"left-token-index\")\n",
       "            })\n",
       "            .attr(\"right-token-index\", (d, i) => i)\n",
       "        ;\n",
       "        updateAttention(svg)\n",
       "    }\n",
       "\n",
       "    function updateAttention(svg) {\n",
       "        svg.select(\"#attention\")\n",
       "            .selectAll(\"line\")\n",
       "            .attr(\"stroke-opacity\", function (d) {\n",
       "                const headIndex = +this.parentNode.parentNode.getAttribute(\"head-index\");\n",
       "                // If head is selected\n",
       "                if (config.headVis[headIndex]) {\n",
       "                    // Set opacity to attention weight divided by number of active heads\n",
       "                    return d / activeHeads()\n",
       "                } else {\n",
       "                    return 0.0;\n",
       "                }\n",
       "            })\n",
       "    }\n",
       "\n",
       "    function boxOffsets(i) {\n",
       "        const numHeadsAbove = config.headVis.reduce(\n",
       "            function (acc, val, cur) {\n",
       "                return val && cur < i ? acc + 1 : acc;\n",
       "            }, 0);\n",
       "        return numHeadsAbove * (BOXWIDTH / activeHeads());\n",
       "    }\n",
       "\n",
       "    function activeHeads() {\n",
       "        return config.headVis.reduce(function (acc, val) {\n",
       "            return val ? acc + 1 : acc;\n",
       "        }, 0);\n",
       "    }\n",
       "\n",
       "    function drawCheckboxes(top, svg) {\n",
       "        const checkboxContainer = svg.append(\"g\");\n",
       "        const checkbox = checkboxContainer.selectAll(\"rect\")\n",
       "            .data(config.headVis)\n",
       "            .enter()\n",
       "            .append(\"rect\")\n",
       "            .attr(\"fill\", (d, i) => headColors(i))\n",
       "            .attr(\"x\", (d, i) => i * CHECKBOX_SIZE)\n",
       "            .attr(\"y\", top)\n",
       "            .attr(\"width\", CHECKBOX_SIZE)\n",
       "            .attr(\"height\", CHECKBOX_SIZE);\n",
       "\n",
       "        function updateCheckboxes() {\n",
       "            checkboxContainer.selectAll(\"rect\")\n",
       "                .data(config.headVis)\n",
       "                .attr(\"fill\", (d, i) => d ? headColors(i): lighten(headColors(i)));\n",
       "        }\n",
       "\n",
       "        updateCheckboxes();\n",
       "\n",
       "        checkbox.on(\"click\", function (d, i) {\n",
       "            if (config.headVis[i] && activeHeads() === 1) return;\n",
       "            config.headVis[i] = !config.headVis[i];\n",
       "            updateCheckboxes();\n",
       "            updateAttention(svg);\n",
       "        });\n",
       "\n",
       "        checkbox.on(\"dblclick\", function (d, i) {\n",
       "            // If we double click on the only active head then reset\n",
       "            if (config.headVis[i] && activeHeads() === 1) {\n",
       "                config.headVis = new Array(config.nHeads).fill(true);\n",
       "            } else {\n",
       "                config.headVis = new Array(config.nHeads).fill(false);\n",
       "                config.headVis[i] = true;\n",
       "            }\n",
       "            updateCheckboxes();\n",
       "            updateAttention(svg);\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function lighten(color) {\n",
       "        const c = d3.hsl(color);\n",
       "        const increment = (1 - c.l) * 0.6;\n",
       "        c.l += increment;\n",
       "        c.s -= increment;\n",
       "        return c;\n",
       "    }\n",
       "\n",
       "    function transpose(mat) {\n",
       "        return mat[0].map(function (col, i) {\n",
       "            return mat.map(function (row) {\n",
       "                return row[i];\n",
       "            });\n",
       "        });\n",
       "    }\n",
       "\n",
       "});"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attention = trace.attn['Layer 0']\n",
    "tokens = trace.get_embeddings(0)\n",
    "input = trace.vocab.human_readable(trace.input)\n",
    "head_view((attention,), input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d', 'd', 'b', 'c', 'b', 'a']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46de579125ce40a59bc70aeeb47f0c79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "import torch\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2Model.from_pretrained(\"gpt2\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a955a5d6d72dcc9422ec8add5a84d6ce2101e50b4aaf9a0a8386fcbc86cc6bec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
